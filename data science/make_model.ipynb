{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAmper Initiation\n",
    "This notebook contains descriptions for the model training procedure.\n",
    "The model.py script learns and outputs the file as per this notebook, which can be run in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection\n",
    "We have preprocessed the file and then done some data cleaning and exploration on the text files. Nothing too in depth, but for the purpose of the excercise, we can move ahead with the modelling procedure.\n",
    "\n",
    "We have two requirements.\n",
    "1. Predict the top 2 class labels.\n",
    "2. Output the relevance measure for each of the labels.\n",
    "\n",
    "Given that we are looking for a relevance score, logistic regression would be a good choice here. Since we are going to ultimately regress the probability that an instance belongs to class k. We won't actually write the algorithm here, since it'll take time to tune it and get the learning rates right. Purpose for this excercise let's just API call sklearns linear model modules. \n",
    "\n",
    "I'm using a stochastic gradient descent optimser here, mainly for extensibility since it supports minibatch descent. So if we scale decide to scale for bigger data sets, we can just change the optimser itself instead of the whole model. \n",
    "\n",
    "Our relevence measure will be the estimated probabilities that the logit function returns for an instance (i) belonging to class (k and j).\n",
    "\n",
    "Lets get on with it.\n",
    "\n",
    "#### Feature extractor / tokenizer\n",
    "Now in the modelling pipeline, we will be feeding in a already cleaned dataset. Which means we can feed it straight into the feature extractor.\n",
    "The requirements have said that the application needs to process a **full string sentence**. This means we first need to feed into a tokenizer to generate tokens. As the data is cleaned, we can use a simple whitespace tokenizer to generate the tokens. From our EDA, we found that some bigrams and trigrams that may be significant. Not real strong, but let's include them in this case.\n",
    "\n",
    "Also, given that we are going to be feeding the runtime feature extracture one instance at a time, tf-idf vectoriser isn't going to be efficient (we will have to feed the training set for each instance and then log each seen instance in the document list, so on so forth). Instead a simple count vectoriser will do the job perfectly here to generate the feature vector. \n",
    "\n",
    "I will use sklearns inbuilt CountVectoriser, which does the whitespace tokenization and some bigram and trigram generation implicitly. We should also keep in mind that the cleaned dataset is stemmed using a prettey aggressive Lancaster stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "_data = pd.read_csv(\"CLEANED_LABELED.csv\")\n",
    "    \n",
    "    \n",
    "# Tokenize and vectorise.\n",
    "_cVectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We will split the data into training and test sets, roughly 70-30 which should suffice. Then we will use the vectoriser that we generated to vectorise the both training and test sets.\n",
    "\n",
    "After training on the vectorised training set. We can predict and see how our mode went.\n",
    "\n",
    "##### Parameters\n",
    "I'm leaving the default hyperparameters to the logistic regression classifier. Although we could do a bit of a hyperparameter search to optimise our mode, for the purpose of this excercise, the default hyperparameters would do. Obviously, the loss function is given as log loss to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84     the inform i nee to do my job is ready avail t...\n",
      "27     i know what is expect from me to be success in...\n",
      "127     my team memb hold each oth account for high qual\n",
      "253    we wer enco to be innov ev though som of our i...\n",
      "134    we held ourselv and our team memb account for ...\n",
      "56                     i understand what is expect of me\n",
      "83     the inform resourc i nee to do my job effect a...\n",
      "136    at pied we hold ourselv and our team memb acco...\n",
      "123    within our branch we hold ourselv account for ...\n",
      "216    we wer enco to be innov ev though som of our i...\n",
      "47     i know what i nee to do to be success in my ro...\n",
      "125               we hold ourselv account for get result\n",
      "203    we ar enco to be innov ev though som of our id...\n",
      "77           the inform nee to do my job effect is ready\n",
      "239                                i am enco to be innov\n",
      "228    we ar enco to be innov or tak risk ev though s...\n",
      "143    my team hold ourselv and our team memb account...\n",
      "248       ev though som in may not i am enco to tak risk\n",
      "124    i hold myself and my team memb account for result\n",
      "146    across we hold ourselv and our team memb accou...\n",
      "59                   i hav the inform i nee to do my job\n",
      "225                               i can tak risk at hool\n",
      "218                               we ar enco to tak risk\n",
      "148       in my team we hold and each account for result\n",
      "19             i know what develop is expect of me in my\n",
      "46                    i know what success look lik in my\n",
      "7            i know what i nee to do to be success in my\n",
      "22               i understand what is expect of me in my\n",
      "242    we ar enco to be innov to meet our goal ev tho...\n",
      "267    we ar enco to be cre and innov ev though som o...\n",
      "                             ...                        \n",
      "109    i hav the inform and resourc i nee to do my jo...\n",
      "232    at ban industry we act on execut innov idea ev...\n",
      "39                  my rol is what i expect it to be lik\n",
      "220    assocy ar enco to be innov ev though som of ou...\n",
      "61         i had access to the inform i nee to do my job\n",
      "97               the inform i nee to do my job was ready\n",
      "169    at pied pip we hold ourselv and our oth team m...\n",
      "172      we hold ourselv and each oth account for result\n",
      "100    i hav access to the inform and resourc i nee t...\n",
      "90     i hav access to the inform and resourc i nee t...\n",
      "215    we ar enco to be innov ev though som of our in...\n",
      "137    we hold ourselv and our team memb account for ...\n",
      "238    we ar enco to be innov ev if som of our in may...\n",
      "31     i know what we nee to do to be success in our ...\n",
      "158    we hold ourselv and our pod memb account for r...\n",
      "181                     my man hold the team account for\n",
      "138              in my team we hold and each account for\n",
      "180            on my we hold each oth account for result\n",
      "17           i understand the object to be success in my\n",
      "186    we hold ourselv and our team memb account for ...\n",
      "6            i understand what is expect of me in my rol\n",
      "208    i am enco to be innov ev though som of the in ...\n",
      "1                  i feel lik i can be success in my rol\n",
      "30                           i know what is expect of me\n",
      "241    we ar enco to be innov ev though som of our in...\n",
      "78     i hav access to the inform and resourc that i ...\n",
      "96     the inform i nee to do my job effect is ready ...\n",
      "244    we ar enco to be risk ev though som of our in ...\n",
      "268                               we ar enco to be innov\n",
      "75     the inform i nee to do my job effect is easy t...\n",
      "Name: question_text, Length: 81, dtype: object ['ENA.3' 'ALI.5' 'TEA.2' 'INN.2' 'TEA.2' 'ALI.5' 'ENA.3' 'TEA.2' 'TEA.2'\n",
      " 'INN.2' 'ALI.5' 'TEA.2' 'INN.2' 'ENA.3' 'INN.2' 'INN.2' 'TEA.2' 'INN.2'\n",
      " 'TEA.2' 'TEA.2' 'ENA.3' 'ALI.5' 'INN.2' 'TEA.2' 'ALI.5' 'ALI.5' 'ALI.5'\n",
      " 'ALI.5' 'INN.2' 'INN.2' 'TEA.2' 'INN.2' 'ALI.5' 'ALI.5' 'INN.2' 'INN.2'\n",
      " 'INN.2' 'TEA.2' 'INN.2' 'TEA.2' 'ENA.3' 'TEA.2' 'TEA.2' 'TEA.2' 'TEA.2'\n",
      " 'ALI.5' 'TEA.2' 'TEA.2' 'ALI.5' 'TEA.2' 'ENA.3' 'ENA.3' 'INN.2' 'ALI.5'\n",
      " 'INN.2' 'ENA.3' 'ENA.3' 'TEA.2' 'TEA.2' 'ENA.3' 'ENA.3' 'INN.2' 'TEA.2'\n",
      " 'INN.2' 'ALI.5' 'TEA.2' 'TEA.2' 'TEA.2' 'TEA.2' 'ALI.5' 'TEA.2' 'ALI.5'\n",
      " 'INN.2' 'ALI.5' 'ALI.5' 'INN.2' 'ENA.3' 'ENA.3' 'INN.2' 'INN.2' 'ENA.3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make test and training data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(_data['question_text'], _data['code'], test_size=0.3,random_state=300)\n",
    "\n",
    "# Fit the vectoriser using training data.\n",
    "_cVectorizer.fit(x_train)\n",
    "\n",
    "# Generate feature vectors\n",
    "fv_train = _cVectorizer.transform(x_train)\n",
    "fv_test = _cVectorizer.transform(x_test)\n",
    "\n",
    "# Logistic regression mode with default parameters\n",
    "_SGDModel = SGDClassifier(loss=\"log\").fit(fv_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = _SGDModel.predict(fv_test)\n",
    "print(x_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
